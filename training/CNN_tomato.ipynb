{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "CHANNELS = 3\n",
    "EPOCHS = 30\n",
    "EXTRACT_PATH = \"PlantVillage\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tomato classes\n",
    "TOMATO_CLASSES = [\n",
    "    'Tomato_Bacterial_spot',\n",
    "    'Tomato_Early_blight',\n",
    "    'Tomato_Late_blight',\n",
    "    'Tomato_Leaf_Mold',\n",
    "    'Tomato_Septoria_leaf_spot',\n",
    "    'Tomato_Spider_mites_Two_spotted_spider_mite',\n",
    "    'Tomato__Target_Spot',\n",
    "    'Tomato__Tomato_YellowLeaf__Curl_Virus',\n",
    "    'Tomato__Tomato_mosaic_virus',\n",
    "    'Tomato_healthy'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: DATASET PREPARATION \n",
    "\n",
    "def prepare_binary_dataset():\n",
    "    \"\"\"\n",
    "    Prepares a dataset for binary classification (Tomato vs. Non-Tomato leaf).\n",
    "    \"\"\"\n",
    "    all_images = []\n",
    "    binary_labels = []\n",
    "    \n",
    "    for class_name in os.listdir(EXTRACT_PATH):\n",
    "        image_dir = os.path.join(EXTRACT_PATH, class_name)\n",
    "        if os.path.isdir(image_dir):\n",
    "            is_tomato = 1 if class_name.startswith('Tomato') else 0\n",
    "            for image_file in os.listdir(image_dir):\n",
    "                if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    image_path = os.path.join(image_dir, image_file)\n",
    "                    img = load_img(image_path, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "                    img_array = img_to_array(img) / 255.0  \n",
    "                    all_images.append(img_array)\n",
    "                    binary_labels.append(is_tomato)\n",
    "\n",
    "    X = np.array(all_images, dtype=np.float32)\n",
    "    y = np.array(binary_labels, dtype=np.float32)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(1000).batch(BATCH_SIZE)\n",
    "    train_size = int(0.8 * len(y))\n",
    "    \n",
    "    train_ds = dataset.take(train_size)\n",
    "    val_ds = dataset.skip(train_size)\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_disease_dataset():\n",
    "    \"\"\"\n",
    "    Prepares a dataset for tomato disease classification.\n",
    "    \"\"\"\n",
    "    tomato_images = []\n",
    "    disease_labels = []\n",
    "\n",
    "    for idx, class_name in enumerate(TOMATO_CLASSES):\n",
    "        image_dir = os.path.join(EXTRACT_PATH, class_name)\n",
    "        if os.path.isdir(image_dir):\n",
    "            for image_file in os.listdir(image_dir):\n",
    "                if image_file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    image_path = os.path.join(image_dir, image_file)\n",
    "                    img = load_img(image_path, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "                    img_array = img_to_array(img) / 255.0  # Normalize\n",
    "                    tomato_images.append(img_array)\n",
    "                    disease_labels.append(idx)\n",
    "\n",
    "    X = np.array(tomato_images, dtype=np.float32)\n",
    "    y = np.array(disease_labels, dtype=np.float32)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(1000).batch(BATCH_SIZE)\n",
    "    train_size = int(0.8 * len(y))\n",
    "\n",
    "    train_ds = dataset.take(train_size)\n",
    "    val_ds = dataset.skip(train_size)\n",
    "\n",
    "    return train_ds, val_ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STEP 2: CREATE MODELS\n",
    "\n",
    "def create_binary_model(input_shape):\n",
    "    \"\"\"\n",
    "    Creates a binary classification model to identify tomato vs non-tomato leaves.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')  \n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_disease_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Creates a multi-class classification model for tomato leaf diseases.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')  \n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "binary_train_ds, binary_val_ds = prepare_binary_dataset()\n",
    "disease_train_ds, disease_val_ds = prepare_disease_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "binary_model = create_binary_model(input_shape)\n",
    "disease_model = create_disease_model(input_shape, len(TOMATO_CLASSES))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9288 - loss: 0.3310"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ab\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m768s\u001b[0m 1s/step - accuracy: 0.9289 - loss: 0.3307\n",
      "Epoch 2/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m667s\u001b[0m 1s/step - accuracy: 0.7787 - loss: 2.0883\n",
      "Epoch 3/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m694s\u001b[0m 1s/step - accuracy: 0.7924 - loss: 1.6742\n",
      "Epoch 4/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m713s\u001b[0m 1s/step - accuracy: 0.7799 - loss: 0.9968\n",
      "Epoch 5/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m662s\u001b[0m 1s/step - accuracy: 0.6413 - loss: 0.6718\n",
      "Epoch 6/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m692s\u001b[0m 1s/step - accuracy: 0.6406 - loss: 0.7072\n",
      "Epoch 7/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m553s\u001b[0m 973ms/step - accuracy: 0.6407 - loss: 0.7373\n",
      "Epoch 8/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m584s\u001b[0m 1s/step - accuracy: 0.6413 - loss: 0.7619\n",
      "Epoch 9/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 1s/step - accuracy: 0.6413 - loss: 0.7829 \n",
      "Epoch 10/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 1s/step - accuracy: 0.6405 - loss: 0.8014 \n",
      "Epoch 11/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m574s\u001b[0m 1s/step - accuracy: 0.6403 - loss: 0.8159 \n",
      "Epoch 12/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m516s\u001b[0m 906ms/step - accuracy: 0.6398 - loss: 0.8283\n",
      "Epoch 13/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m556s\u001b[0m 977ms/step - accuracy: 0.6411 - loss: 0.8348\n",
      "Epoch 14/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m571s\u001b[0m 1s/step - accuracy: 0.6407 - loss: 0.8432 \n",
      "Epoch 15/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 969ms/step - accuracy: 0.6410 - loss: 0.8486\n",
      "Epoch 16/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 970ms/step - accuracy: 0.6411 - loss: 0.8535\n",
      "Epoch 17/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m543s\u001b[0m 956ms/step - accuracy: 0.6414 - loss: 0.8569\n",
      "Epoch 18/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 1s/step - accuracy: 0.6406 - loss: 0.8618\n",
      "Epoch 19/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m585s\u001b[0m 1s/step - accuracy: 0.6409 - loss: 0.8640 \n",
      "Epoch 20/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m603s\u001b[0m 1s/step - accuracy: 0.6409 - loss: 0.8659\n",
      "Epoch 21/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m585s\u001b[0m 1s/step - accuracy: 0.6410 - loss: 0.8675 \n",
      "Epoch 22/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m588s\u001b[0m 1s/step - accuracy: 0.6408 - loss: 0.8695 \n",
      "Epoch 23/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m693s\u001b[0m 1s/step - accuracy: 0.6422 - loss: 0.8678\n",
      "Epoch 24/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m676s\u001b[0m 1s/step - accuracy: 0.6414 - loss: 0.8704\n",
      "Epoch 25/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m767s\u001b[0m 1s/step - accuracy: 0.6410 - loss: 0.8721\n",
      "Epoch 26/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m746s\u001b[0m 1s/step - accuracy: 0.6406 - loss: 0.8735\n",
      "Epoch 27/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m690s\u001b[0m 1s/step - accuracy: 0.6416 - loss: 0.8720\n",
      "Epoch 28/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m743s\u001b[0m 1s/step - accuracy: 0.6410 - loss: 0.8736\n",
      "Epoch 29/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m746s\u001b[0m 1s/step - accuracy: 0.6409 - loss: 0.8741\n",
      "Epoch 30/30\n",
      "\u001b[1m568/568\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m635s\u001b[0m 1s/step - accuracy: 0.6413 - loss: 0.8737\n",
      "Epoch 1/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - accuracy: 0.5800 - loss: 1.3247\n",
      "Epoch 2/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.9978 - loss: 0.0569   \n",
      "Epoch 3/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.9922 - loss: 0.0995\n",
      "Epoch 4/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0183\n",
      "Epoch 5/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.9922 - loss: 0.1416\n",
      "Epoch 6/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 0.9948 - loss: 0.0102\n",
      "Epoch 7/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0070\n",
      "Epoch 8/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0038\n",
      "Epoch 9/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 10/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.7799e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 4.0357e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 2.4597e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 2.1274e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 6.8758e-05\n",
      "Epoch 15/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.6299e-05\n",
      "Epoch 16/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.1347e-05\n",
      "Epoch 17/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.4750e-05\n",
      "Epoch 18/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 4.1505e-05\n",
      "Epoch 19/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.3093e-05\n",
      "Epoch 20/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 2.8055e-05\n",
      "Epoch 21/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.2730e-05\n",
      "Epoch 22/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 4.9506e-06\n",
      "Epoch 23/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 4.8988e-06\n",
      "Epoch 24/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.7059e-06\n",
      "Epoch 25/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.0684e-06\n",
      "Epoch 26/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.1702e-06\n",
      "Epoch 27/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 9.7375e-06\n",
      "Epoch 28/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 3.5187e-06\n",
      "Epoch 29/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 2.0150e-06\n",
      "Epoch 30/30\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 7.7274e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24bde8e56f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the binary model\n",
    "binary_model.fit(binary_train_ds, validation_data=binary_val_ds, epochs=EPOCHS)\n",
    "\n",
    "# Train the disease classification model\n",
    "disease_model.fit(disease_train_ds, validation_data=disease_val_ds, epochs=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the binary classification model\n",
    "binary_model.save(\"../saved-models/binary_model.keras\")\n",
    "\n",
    "# Save the disease classification model\n",
    "disease_model.save(\"../saved-models/disease_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 4: PREDICTION PIPELINE\n",
    "\n",
    "def classify_image(image_path):\n",
    "    \"\"\"\n",
    "    Predicts whether the image is a tomato leaf and, if so, classifies its disease.\n",
    "    \"\"\"\n",
    "    img = load_img(image_path, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)  \n",
    "\n",
    "    # Step 1: Check if it's a tomato leaf\n",
    "    tomato_prob = binary_model.predict(img_array)[0][0]\n",
    "\n",
    "    if tomato_prob < 0.5:\n",
    "        return \"Non-tomato leaf\"\n",
    "\n",
    "    # Step 2: Predict disease classification\n",
    "    disease_prediction = disease_model.predict(img_array)\n",
    "    disease_class = np.argmax(disease_prediction)\n",
    "    \n",
    "    return TOMATO_CLASSES[disease_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Prediction: Tomato_Late_blight\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_path = './PlantVillage/Tomato_Late_blight/005a2c1f-4e15-49e4-9e5c-61dc3ecf9708___RS_Late.B 5096.JPG'\n",
    "result = classify_image(image_path)\n",
    "print(\"Prediction:\", result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
